# coding: utf-8
#  Advanced: Making Dynamic Decisions and the Bi-LSTM CRF

import torch
import torch.nn as nn
import torch.optim as optim
import datetime as dt
import pandas as pd
import time
import csv
import sys
maxInt = sys.maxsize
decrement = True

torch.manual_seed(1)


def argmax(vec):
    # return the argmax as a python int
    _, idx = torch.max(vec, 1)
    return idx.item()


def prepare_sequence(seq, to_ix):
    # idxs = [to_ix[w] for w in seq]
    idxs = []
    for w in seq:
        # print(w)
        idxs.append(to_ix[w])
    return torch.tensor(idxs, dtype=torch.long)


# Compute log sum exp in a numerically stable way for the forward algorithm
def log_sum_exp(vec):
    max_score = vec[0, argmax(vec)]
    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])
    return max_score + \
        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))

#####################################################################
# Create model


class BiLSTM_CRF(nn.Module):

    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):
        super(BiLSTM_CRF, self).__init__()
        self.embedding_dim = embedding_dim  # 词向量维度
        self.hidden_dim = hidden_dim
        self.vocab_size = vocab_size
        self.tag_to_ix = tag_to_ix
        self.tagset_size = len(tag_to_ix)

        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,
                            num_layers=1, bidirectional=True)

        # Maps the output of the LSTM into tag space.
        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)

        # Matrix of transition parameters.  Entry i,j is the score of
        # transitioning *to* i *from* j.
        self.transitions = nn.Parameter(
            torch.randn(self.tagset_size, self.tagset_size))

        # These two statements enforce the constraint that we never transfer
        # to the start tag and we never transfer from the stop tag
        self.transitions.data[tag_to_ix[START_TAG], :] = -10000
        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000

        self.hidden = self.init_hidden()

    def init_hidden(self):
        return (torch.randn(2, 1, self.hidden_dim // 2),
                torch.randn(2, 1, self.hidden_dim // 2))

    def _forward_alg(self, feats):
        # Do the forward algorithm to compute the partition function
        init_alphas = torch.full((1, self.tagset_size), -10000.)
        # START_TAG has all of the score.
        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.

        # Wrap in a variable so that we will get automatic backprop
        forward_var = init_alphas

        # Iterate through the sentence
        for feat in feats:
            alphas_t = []  # The forward tensors at this timestep
            for next_tag in range(self.tagset_size):
                emit_score = feat[next_tag].view(
                    1, -1).expand(1, self.tagset_size)
                trans_score = self.transitions[next_tag].view(1, -1)
                next_tag_var = forward_var + trans_score + emit_score
                alphas_t.append(log_sum_exp(next_tag_var).view(1))
            forward_var = torch.cat(alphas_t).view(1, -1)
        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]
        alpha = log_sum_exp(terminal_var)
        return alpha

    def _get_lstm_features(self, sentence):
        self.hidden = self.init_hidden()
        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)
        lstm_out, self.hidden = self.lstm(embeds, self.hidden)
        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)
        lstm_feats = self.hidden2tag(lstm_out)
        return lstm_feats

    def _score_sentence(self, feats, tags):
        # Gives the score of a provided tag sequence
        score = torch.zeros(1)
        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])
        for i, feat in enumerate(feats):
            score = score + \
                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]
        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]
        return score

    def _viterbi_decode(self, feats):
        backpointers = []

        # Initialize the viterbi variables in log space
        init_vvars = torch.full((1, self.tagset_size), -10000.)
        init_vvars[0][self.tag_to_ix[START_TAG]] = 0

        # forward_var at step i holds the viterbi variables for step i-1
        forward_var = init_vvars
        for feat in feats:
            bptrs_t = []  # holds the backpointers for this step
            viterbivars_t = []  # holds the viterbi variables for this step

            for next_tag in range(self.tagset_size):
                next_tag_var = forward_var + self.transitions[next_tag]
                best_tag_id = argmax(next_tag_var)
                bptrs_t.append(best_tag_id)
                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))
            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)
            backpointers.append(bptrs_t)

        # Transition to STOP_TAG
        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]
        best_tag_id = argmax(terminal_var)
        path_score = terminal_var[0][best_tag_id]

        # Follow the back pointers to decode the best path.
        best_path = [best_tag_id]
        for bptrs_t in reversed(backpointers):
            best_tag_id = bptrs_t[best_tag_id]
            best_path.append(best_tag_id)
        # Pop off the start tag (we dont want to return that to the caller)
        start = best_path.pop()
        assert start == self.tag_to_ix[START_TAG]  # Sanity check
        best_path.reverse()
        return path_score, best_path

    def neg_log_likelihood(self, sentence, tags):
        feats = self._get_lstm_features(sentence)
        forward_score = self._forward_alg(feats)
        gold_score = self._score_sentence(feats, tags)
        return forward_score - gold_score

    def forward(self, sentence):  # dont confuse this with _forward_alg above.
        # Get the emission scores from the BiLSTM
        lstm_feats = self._get_lstm_features(sentence)

        # Find the best path, given the features.
        score, tag_seq = self._viterbi_decode(lstm_feats)
        return score, tag_seq

#####################################################################
# Run training

START_TAG = "<START>"
STOP_TAG = "<STOP>"
EMBEDDING_DIM = 5
HIDDEN_DIM = 4


def run_training(epochs, model_name, data_divide):
    # Make up some training data
    # bs_training_data = [(
    #     "姜文的侠隐也变成了2019贾樟柯的江湖儿女",
    #     "OOOBIOOOOOOOOOOOOBIII"
    # ), (
    #     "2019好在还有娄烨的地狱恋人今年的年度的期待",
    #     "OOOOOOOOOOOBIIIOOOOOOOO"
    # )]

    bs_training_data = []

    bs_format_data = pd.read_csv('format_data_UTF8.csv')
    bs_sentence_data = bs_format_data['content']
    bs_tags_data = bs_format_data['value']
    for count in range(int(len(bs_sentence_data) / data_divide)):
        item = (bs_sentence_data[count], bs_tags_data[count])
        bs_training_data.append(item)
    print(bs_training_data)

    datetime_start = dt.datetime.now()
    print('start: ' + datetime_start.strftime('%Y-%m-%d %H:%M:%S'))

    test_data = [
        "2019贾樟柯的《江湖儿女》2019陈冲的《英格力士》2019好在还有娄烨的《地狱恋人》"
    ]

    word_to_ix = {}
    for sentence in bs_sentence_data:
        for word in sentence:
            if word not in word_to_ix:
                word_to_ix[word] = len(word_to_ix)

    # for sentence in test_data:
    #     for word in sentence:
    #         if word not in word_to_ix:
    #             word_to_ix[word] = len(word_to_ix)

    tag_to_ix = {"B": 0, "I": 1, "O": 2, START_TAG: 3, STOP_TAG: 4}

    model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)
    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)

    datetime_epoch = dt.datetime.now()
    print('start epoch: ' + datetime_epoch.strftime('%Y-%m-%d %H:%M:%S'))

    # Check predictions before training
    # with torch.no_grad():
    #     precheck_sent = prepare_sequence(test_data[0], word_to_ix)
    #     # precheck_tags = torch.tensor([tag_to_ix[t] for t in bs_training_data[0][1]], dtype=torch.long)
    #     print(model(precheck_sent))

    # Make sure prepare_sequence from earlier in the LSTM section is loaded
    for epoch in range(
            epochs):  # again, normally you would NOT do 300 epochs, it is toy data
        datetime_epoch = dt.datetime.now()
        print("epoch:" + str(epoch) + "    " + datetime_epoch.strftime('%Y-%m-%d %H:%M:%S'))

        for sentence, tags in bs_training_data:

            # Step 1. Remember that Pytorch accumulates gradients.
            # We need to clear them out before each instance
            model.zero_grad()

            # Step 2. Get our inputs ready for the network, that is,
            # turn them into Tensors of word indices.
            sentence_in = prepare_sequence(sentence, word_to_ix)
            targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)

            # Step 3. Run our forward pass.
            loss = model.neg_log_likelihood(sentence_in, targets)

            # Step 4. Compute the loss, gradients, and update the parameters by
            # calling optimizer.step()
            loss.backward()
            optimizer.step()

    datetime_predict = dt.datetime.now()
    print('end: ' + datetime_predict.strftime('%Y-%m-%d %H:%M:%S'))

    torch.save(model, model_name + '.pkl')
    # Check predictions after training
    with torch.no_grad():
        # precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)
        # check_tags = torch.tensor([tag_to_ix[t] for t in bs_training_data[1][1]], dtype=torch.long)
        for sentence in test_data:
            print(sentence)
            check_sent = prepare_sequence(sentence, word_to_ix)
            print(model(check_sent))
            # predict_tag = model(check_sent)[1]
            # word_list = []
            # temp_word = []
            # for tag_i in range(len(predict_tag)):
            #     if predict_tag[tag_i] == 0:
            #         temp_word.append(sentence[tag_i])
            #     elif predict_tag[tag_i] == 1 and len(temp_word) != 0:
            #         temp_word.append(sentence[tag_i])
            #     elif predict_tag[tag_i] == 2 and len(temp_word) != 0:
            #         word_list.append(''.join(temp_word))
            #         temp_word.clear()
            # if len(temp_word) != 0:
            #     word_list.append(temp_word)
            #     temp_word.clear()
            # print(word_list)


def load_model(model_name):
    model = torch.load(model_name + '.pkl')
    test_data = [
        # "还有娄烨的地狱恋人今年的期待",
        # "我喜欢的电影是江湖儿女",
        # "英雄儿女是部好电影",
        # "人生是江湖儿女情长",
        # "侠隐是电影江湖儿女的导演吗",
        "你哭了无问西东吗",
        # "今天你去看《人在囧途》了吗"
    ]

    bs_format_data = pd.read_csv('format_data_new.csv')
    test_sentence_data = bs_format_data['content']
    test_tags_data = bs_format_data['value']
    test_training_data = []
    for count in range(int(len(test_sentence_data) / 2), int(len(test_sentence_data))):
        item = (test_sentence_data[count], test_tags_data[count])
        test_training_data.append(item)
    # print(test_training_data)

    word_format_data = pd.read_csv('format_data_new.csv')
    word_sentence_data = word_format_data['content']
    word_to_ix = {}
    for sentence in word_sentence_data:
        for word in sentence:
            if word not in word_to_ix:
                word_to_ix[word] = len(word_to_ix)

    accurate = 0
    count = 0
    count_true = 0
    TP = 0
    FP = 0  #
    TN = 0  # 恒为0
    TP_TN = 0
    for sentence in test_data:
        print(sentence)
        check_sent = prepare_sequence(sentence, word_to_ix)
        print(model(check_sent))
    for sentence, tags in test_training_data:
        count += 1
        if sentence.find('B') != -1:
            TP_TN += 1

        # print(sentence)
        check_sent = prepare_sequence(sentence, word_to_ix)
        predict_result = model(check_sent)
        predict_BIO_list = predict_result[1]
        predict_BIO = ''
        for item in predict_BIO_list:
            temp = ''
            if item == 0:
                temp = 'B'
            elif item == 1:
                temp = 'I'
            elif item == 2:
                temp = 'O'
            else:
                temp = 'O'
            predict_BIO += temp
        if tags.find('B') != -1:
            count_true += 1
        if tags == predict_BIO:
            accurate += 1
            if tags.find('B') != -1:
                TP += 1
                print(sentence)
                print(predict_BIO)
        # elif tags.find('B') == -1 and predict_BIO.find('B') != -1:
        elif tags != predict_BIO and predict_BIO.find('B') != -1:
                FP += 1

    print(model_name + '.pkl')
    print('count:' + str(count) + '; ' + 'count_true:' + str(count_true) + '; ' + 'accurate_count:' + str(accurate) + '; ' + str(accurate / count))
    print('TP:' + str(TP) + '; ' + 'FP:' + str(FP))


while decrement:
    decrement = False
    try:
        csv.field_size_limit(maxInt)
    except OverflowError:
        maxInt = int(maxInt / 10)
        decrement = True


def run_BEMS_training(epochs):
    bs_training_data = []
    bs_sentence = ''
    bs_tags = ''
    with open("BMEStrain.data", "r", encoding="utf-8") as csvfile:
        read = csv.reader(csvfile)
        for item in read:
            if len(item) != 0:
                if len(item[0]) != 0:
                    bs_sentence += item[0][0]
                    if len(item[0]) > 2:
                        bs_tags_data = item[0][2]
                    else:
                        bs_tags_data = item[0][len(item) - 1]
                    if bs_tags_data == 'B':
                        bs_tags += 'B'
                    elif bs_tags_data == 'M' or bs_tags_data == 'E':
                        bs_tags += 'I'
                    else:
                        bs_tags += 'O'
            else:
                train_item = (bs_sentence, bs_tags)
                bs_training_data.append(train_item)
                bs_sentence = ''
                bs_tags = ''
    print(bs_training_data)

    bs_test_data = []
    bs_sentence = ''
    bs_tags = ''
    with open("BMEStest.data", "r", encoding="utf-8") as csvfile:
        read = csv.reader(csvfile)
        for item in read:
            if len(item) != 0:
                if len(item[0]) != 0:
                    bs_sentence += item[0][0]
                    if len(item[0]) > 2:
                        bs_tags_data = item[0][2]
                    else:
                        bs_tags_data = item[0][len(item) - 1]
                    if bs_tags_data == 'B':
                        bs_tags += 'B'
                    elif bs_tags_data == 'M' or bs_tags_data == 'E':
                        bs_tags += 'I'
                    else:
                        bs_tags += 'O'
            else:
                train_item = (bs_sentence, bs_tags)
                bs_test_data.append(train_item)
                bs_sentence = ''
                bs_tags = ''
    print(bs_test_data)

    datetime_start = dt.datetime.now()
    print('start: ' + datetime_start.strftime('%Y-%m-%d %H:%M:%S'))

    test_data = [
        "2019贾樟柯的江湖儿女2019陈冲的《英格力士》2019好在还有地狱恋人"
    ]

    word_to_ix = {}
    for sentence, tags in bs_training_data:
        for word in sentence:
            if word not in word_to_ix:
                word_to_ix[word] = len(word_to_ix)

    for sentence, tags in bs_test_data:
        for word in sentence:
            if word not in word_to_ix:
                word_to_ix[word] = len(word_to_ix)

    tag_to_ix = {"B": 0, "I": 1, "O": 2, START_TAG: 3, STOP_TAG: 4}

    model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)
    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)

    datetime_epoch = dt.datetime.now()
    print('start epoch: ' + datetime_epoch.strftime('%Y-%m-%d %H:%M:%S'))

    # Make sure prepare_sequence from earlier in the LSTM section is loaded
    for epoch in range(
            epochs):  # again, normally you would NOT do 300 epochs, it is toy data
        datetime_epoch = dt.datetime.now()
        print("epoch:" + str(epoch) + "    " + datetime_epoch.strftime('%Y-%m-%d %H:%M:%S'))

        for sentence, tags in bs_training_data:
            # Step 1. Remember that Pytorch accumulates gradients.
            # We need to clear them out before each instance
            model.zero_grad()

            # Step 2. Get our inputs ready for the network, that is,
            # turn them into Tensors of word indices.
            sentence_in = prepare_sequence(sentence, word_to_ix)
            targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)

            # Step 3. Run our forward pass.
            loss = model.neg_log_likelihood(sentence_in, targets)

            # Step 4. Compute the loss, gradients, and update the parameters by
            # calling optimizer.step()
            loss.backward()
            optimizer.step()

    datetime_predict = dt.datetime.now()
    print('end: ' + datetime_predict.strftime('%Y-%m-%d %H:%M:%S'))

    torch.save(model, 'model_BMES_' + str(epochs) + '.pkl')
    # Check predictions after training
    with torch.no_grad():
        for sentence in test_data:
            print(sentence)
            check_sent = prepare_sequence(sentence, word_to_ix)
            print(model(check_sent))


def run_test(epochs):
    model = torch.load('model_BMES_' + str(epochs) + '.pkl')
    bs_test_data = []
    bs_sentence = ''
    bs_tags = ''
    with open("BMEStest.data", "r", encoding="utf-8") as csvfile:
        read = csv.reader(csvfile)
        for item in read:
            if len(item) != 0:
                if len(item[0]) != 0:
                    bs_sentence += item[0][0]
                    if len(item[0]) > 2:
                        bs_tags_data = item[0][2]
                    else:
                        bs_tags_data = item[0][len(item) - 1]
                    if bs_tags_data == 'B':
                        bs_tags += 'B'
                    elif bs_tags_data == 'M' or bs_tags_data == 'E':
                        bs_tags += 'I'
                    else:
                        bs_tags += 'O'
            else:
                train_item = (bs_sentence, bs_tags)
                bs_test_data.append(train_item)
                bs_sentence = ''
                bs_tags = ''
    print(bs_test_data)

    # vector
    bs_training_data = []
    bs_sentence = ''
    bs_tags = ''
    with open("BMEStrain.data", "r", encoding="utf-8") as csvfile:
        read = csv.reader(csvfile)
        for item in read:
            if len(item) != 0:
                if len(item[0]) != 0:
                    bs_sentence += item[0][0]
                    if len(item[0]) > 2:
                        bs_tags_data = item[0][2]
                    else:
                        bs_tags_data = item[0][len(item) - 1]
                    if bs_tags_data == 'B':
                        bs_tags += 'B'
                    elif bs_tags_data == 'M' or bs_tags_data == 'E':
                        bs_tags += 'I'
                    else:
                        bs_tags += 'O'
            else:
                train_item = (bs_sentence, bs_tags)
                bs_training_data.append(train_item)
                bs_sentence = ''
                bs_tags = ''
    word_to_ix = {}
    for sentence, tags in bs_training_data:
        for word in sentence:
            if word not in word_to_ix:
                word_to_ix[word] = len(word_to_ix)
    for sentence, tags in bs_test_data:
        for word in sentence:
            if word not in word_to_ix:
                word_to_ix[word] = len(word_to_ix)

    count_entity = 0
    count_sentence = 0
    predict_entities = []
    test_BIOs = []
    predict_count = 0
    bs_test_data_count = len(bs_test_data)
    bs_test_data_group = int(bs_test_data_count / 5)
    print('bs_test_data_count:' + str(bs_test_data_count))
    # for group_i in range(0, bs_test_data_group):
    #     group_start = group_i * 5
    #     group_end = (group_i + 1) * 5
    for count_i in range(bs_test_data_count):
        if count_i >= bs_test_data_count:
            break
        sentence = bs_test_data[count_i][0]
        tags = bs_test_data[count_i][1]
    # for sentence, tags in bs_test_data:
        count_sentence += 1
        if tags.find('B') != -1:
            count_entity += tags.count('B')
        # predict
        print(sentence)
        with torch.no_grad():
            check_sent = prepare_sequence(sentence, word_to_ix)
            predict_result = model(check_sent)
            predict_BIO_list = predict_result[1]  # {1,2,2,2,2,2,2}
            predict_BIO = []  # {位置}
            for item_i in range(len(predict_BIO_list)):
                if predict_BIO_list[item_i] == 0:  # B
                    predict_count += 1
                    predict_BIO.append(sentence[item_i])
                elif predict_BIO_list[item_i] == 1:  # I
                    predict_BIO.append(sentence[item_i])
                else:
                    if len(predict_BIO) != 0:
                        str_BIO = ''.join(predict_BIO)
                        predict_entities.append(str_BIO)
                        predict_BIO.clear()
            if len(predict_BIO) != 0:
                predict_entities.append(predict_BIO)
                predict_BIO.clear()
            print('predict_entities:' + str(len(predict_entities)))
            print(predict_entities)
        # test data
        test_BIO = []
        for tags_i in range(len(tags)):
            if tags[tags_i] == 'B':
                test_BIO.append(sentence[tags_i])
            elif tags[tags_i] == 'I':
                test_BIO.append(sentence[tags_i])
            else:
                if len(test_BIO) != 0:
                    test_BIOs.append(''.join(test_BIO))
                    test_BIO.clear()
        if len(test_BIO) != 0:
            test_BIOs.append(test_BIO)
            test_BIO.clear()
        print('test_BIOs:' + str(len(test_BIOs)))
        print(test_BIOs)
            # calculate
            # if ()
        # time.sleep(5)


    print(str(epochs) + '.pkl')
    print('count_sentence:' + str(count_sentence) + '; count_entity:' + str(count_entity) + ' ;test_BIOs:' + str(len(test_BIOs)) + ' ;predict_count:' + str(predict_count))
    # print('count:' + str(count) + '; ' + 'count_true:' + str(count_true) + '; ' + 'accurate_count:' + str(
    #     accurate) + '; ' + str(accurate / count))

    TP = 0  # 正确
    FP = 0  # 其他分类 识别出的
    TN = 0  # 不是反而被识别出的
    FN = 0  # 漏报
    for item in predict_entities:
        if item in test_BIOs:
            TP += 1  # 将正类预测为正类数
        else:
            FP += 1  # 将负类预测为正类数(误报)
    for item in test_BIOs:
        if item not in predict_entities:
            FN += 1  # 将正类预测为负类

    P_score = 0.00
    R_score = 0.00
    F_score = 0.00
    if TP + FP != 0:
        P_score = TP / (TP + FP)
    if TP + FN != 0:
        R_score = TP / (TP + FN)
    if P_score + R_score != 0:
        F_score = 2 * P_score * R_score / (P_score + R_score)
    print('TP:' + str(TP) + '; ' + 'FP:' + str(FP) + '; ' + 'FN:' + str(FN))
    print('P:' + str(P_score) + '; ' + 'R:' + str(R_score) + '; ' + 'F-value:' + str(F_score))


def run_model(model_name):
    model = torch.load('model_BMES_' + str(model_name) + '.pkl')
    bs_test_data = []
    bs_sentence = ''
    bs_tags = ''
    with open("BMEStest.data", "r", encoding="utf-8") as csvfile:
        read = csv.reader(csvfile)
        for item in read:
            if len(item) != 0:
                if len(item[0]) != 0:
                    bs_sentence += item[0][0]
                    if len(item[0]) > 2:
                        bs_tags_data = item[0][2]
                    else:
                        bs_tags_data = item[0][len(item) - 1]
                    if bs_tags_data == 'B':
                        bs_tags += 'B'
                    elif bs_tags_data == 'M' or bs_tags_data == 'E':
                        bs_tags += 'I'
                    else:
                        bs_tags += 'O'
            else:
                train_item = (bs_sentence, bs_tags)
                bs_test_data.append(train_item)
                bs_sentence = ''
                bs_tags = ''
    print(bs_test_data)

    # vector
    bs_training_data = []
    bs_sentence = ''
    bs_tags = ''
    with open("BMEStrain.data", "r", encoding="utf-8") as csvfile:
        read = csv.reader(csvfile)
        for item in read:
            if len(item) != 0:
                if len(item[0]) != 0:
                    bs_sentence += item[0][0]
                    if len(item[0]) > 2:
                        bs_tags_data = item[0][2]
                    else:
                        bs_tags_data = item[0][len(item) - 1]
                    if bs_tags_data == 'B':
                        bs_tags += 'B'
                    elif bs_tags_data == 'M' or bs_tags_data == 'E':
                        bs_tags += 'I'
                    else:
                        bs_tags += 'O'
            else:
                train_item = (bs_sentence, bs_tags)
                bs_training_data.append(train_item)
                bs_sentence = ''
                bs_tags = ''
    word_to_ix = {}
    for sentence, tags in bs_training_data:
        for word in sentence:
            if word not in word_to_ix:
                word_to_ix[word] = len(word_to_ix)
    for sentence, tags in bs_test_data:
        for word in sentence:
            if word not in word_to_ix:
                word_to_ix[word] = len(word_to_ix)

    count_entity = 0
    count_sentence = 0
    predict_entities = []
    test_BIOs = []
    predict_count = 0
    bs_test_data_count = len(bs_test_data)
    bs_test_data_group = int(bs_test_data_count / 5)
    print('bs_test_data_count:' + str(bs_test_data_count))
    # for group_i in range(0, bs_test_data_group):
    #     group_start = group_i * 5
    #     group_end = (group_i + 1) * 5
    read_data = pd.read_csv('test_BMES.csv')
    test_sentences = read_data['content']
    test_tags = read_data['value']
    for sentence_i in range(len(test_sentences)):
        sentence = test_sentences[sentence_i]
        tags = test_tags[sentence_i]
        count_sentence += 1
        if tags.find('B') != -1:
            count_entity += tags.count('B')
        # predict
        print(sentence)
        with torch.no_grad():
            check_sent = prepare_sequence(sentence, word_to_ix)
            predict_result = model(check_sent)
            predict_BIO_list = predict_result[1]  # {1,2,2,2,2,2,2}
            predict_BIO = []  # {位置}
            for item_i in range(len(predict_BIO_list)):
                if predict_BIO_list[item_i] == 0:  # B
                    predict_count += 1
                    predict_BIO.append(sentence[item_i])
                elif predict_BIO_list[item_i] == 1:  # I
                    predict_BIO.append(sentence[item_i])
                else:
                    if len(predict_BIO) != 0:
                        str_BIO = ''.join(predict_BIO)
                        predict_entities.append(str_BIO)
                        predict_BIO.clear()
            if len(predict_BIO) != 0:
                predict_entities.append(predict_BIO)
                predict_BIO.clear()
            print('predict_entities:' + str(len(predict_entities)))
            print(predict_entities)
        # test data
        test_BIO = []
        for tags_i in range(len(tags)):
            if tags[tags_i] == 'B':
                test_BIO.append(sentence[tags_i])
            elif tags[tags_i] == 'I':
                test_BIO.append(sentence[tags_i])
            else:
                if len(test_BIO) != 0:
                    test_BIOs.append(''.join(test_BIO))
                    test_BIO.clear()
        if len(test_BIO) != 0:
            test_BIOs.append(test_BIO)
            test_BIO.clear()
        print('test_BIOs:' + str(len(test_BIOs)))
        print(test_BIOs)
        # calculate
        # if ()
        # time.sleep(5)

    print(str(model_name) + '.pkl')
    print('count_sentence:' + str(count_sentence) + '; count_entity:' + str(count_entity) + ' ;test_BIOs:' + str(
        len(test_BIOs)) + ' ;predict_count:' + str(predict_count))
    # print('count:' + str(count) + '; ' + 'count_true:' + str(count_true) + '; ' + 'accurate_count:' + str(
    #     accurate) + '; ' + str(accurate / count))

    TP = 0  # 正确
    FP = 0  # 其他分类 识别出的, 0
    TN = 0  # 不是反而被识别出的
    for item in predict_entities:
        if item in test_BIOs:
            TP += 1
        else:
            TN += 1

    P_score = 0.00
    R_score = 0.00
    F_score = 0.00
    if TP + FP != 0:
        P_score = TP / (TP + FP)
    if TP + TN != 0:
        R_score = TP / (TP + TN)
    if P_score + R_score != 0:
        F_score = 2 * P_score * R_score / (P_score + R_score)
    print('TP:' + str(TP) + '; ' + 'TN:' + str(TN))
    print('P:' + str(P_score) + '; ' + 'R:' + str(R_score) + '; ' + 'F-value:' + str(F_score))


if __name__ == '__main__':
    # run_training(10, 'model_2_10_no', 2)
    # load_model('model_50_50_no')
    # load_model('model_20_50')
    # run_BEMS_training(20)
    run_test(20)
    # run_model(8)

# We got it!


######################################################################
# Exercise: A new loss function for discriminative tagging
# --------------------------------------------------------
#
# It wasn't really necessary for us to create a computation graph when
# doing decoding, since we do not backpropagate from the viterbi path
# score. Since we have it anyway, try training the tagger where the loss
# function is the difference between the Viterbi path score and the score
# of the gold-standard path. It should be clear that this function is
# non-negative and 0 when the predicted tag sequence is the correct tag
# sequence. This is essentially *structured perceptron*.
#
# This modification should be short, since Viterbi and score\_sentence are
# already implemented. This is an example of the shape of the computation
# graph *depending on the training instance*. Although I haven't tried
# implementing this in a static toolkit, I imagine that it is possible but
# much less straightforward.
#
# Pick up some real data and do a comparison!
#